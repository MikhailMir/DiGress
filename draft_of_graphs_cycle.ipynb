{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from src.diffusion_model_discrete import DiscreteDenoisingDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../graph_diversity_problems/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generation import get_initial_graphs, precompute_assets_for_generated_graphs\n",
    "from base import DiversityBaseClass\n",
    "from utils import read_pickle\n",
    "\n",
    "from analysis import draw_graphs\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "from distances import DISTANCE_TO_FUNC\n",
    "from pathlib import Path\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from distances import ProgressParallel\n",
    "\n",
    "from typing import List, Tuple, Any, Callable, Dict, Optional\n",
    "\n",
    "from itertools import combinations\n",
    "from base import GraphObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# path = input()\n",
    "# print(path)\n",
    "# model = DiscreteDenoisingDiffusion.load_from_checkpoint(path, map_location=\"cpu\")\n",
    "# model.visualization_tools = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_state_dict_from_other_run(ckpt_path):\n",
    "#     model = DiscreteDenoisingDiffusion.load_from_checkpoint(ckpt_path, map_location=\"cpu\").model\n",
    "#     return model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_objects(adjacencies=None,\n",
    "                      get_initial_graphs_func=None, \n",
    "                      distances_set={\"netLSD_heat\", \"netLSD_wave\", \"GCD\", \"Portrait\"}):\n",
    "    \n",
    "    config = {\n",
    "        \"initial_graphs\": \"user\",\n",
    "    }\n",
    "    \n",
    "    if adjacencies is not None and get_initial_graphs_func is None:\n",
    "        \n",
    "        graph_with_computed_descriptors = get_initial_graphs(\n",
    "                                                config=config,\n",
    "                                                threads=12, \n",
    "                                                distances_set=distances_set, \n",
    "                                                samples=None, \n",
    "                                                nodes_number=16, \n",
    "                                                orca_path=\"../graph_diversity_problems/orca/\", \n",
    "                                                equal_sizes=True, \n",
    "                                                maybe_ready_graphs=adjacencies,\n",
    "                                                greedy_graphs_objects_per_distance=None,\n",
    "                                            )[\"user\"]\n",
    "    elif adjacencies is None and get_initial_graphs_func is not None:\n",
    "        graph_with_computed_descriptors = get_initial_graphs_func()[\"mix\"]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Either adjacencies or graph_with_computed_descriptors should be specified!\")\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    for distance, graph_label_entity in graph_with_computed_descriptors.items():\n",
    "        graph_objects = [\n",
    "            GraphObject(\n",
    "                _entity=e, identification=i, _graph=g\n",
    "            ) for g, i, e in graph_label_entity\n",
    "        ]\n",
    "        \n",
    "        result_dict[distance] = graph_objects\n",
    "        \n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pairwise_energy(graphs: List[GraphObject], distance_function:Callable[[Any, Any], float]):\n",
    "    i_j_indices = combinations(graphs, 2)\n",
    "    \n",
    "    distances = ProgressParallel(n_jobs=12)(\n",
    "        delayed(\n",
    "            distance_function\n",
    "        )(e_1, e_2) for e_1, e_2 in i_j_indices\n",
    "    )\n",
    "    \n",
    "    distances = np.array(distances)\n",
    "    return distances, distances.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_distance(x: GraphObject, y: GraphObject, distance_name:str):\n",
    "    return 1 / (DISTANCE_TO_FUNC[distance_name](x.entity, y.entity) + 1e-6)\n",
    "\n",
    "PORTRAIT = partial(energy_distance, distance_name=\"Portrait\")\n",
    "GCD = partial(energy_distance, distance_name=\"GCD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graphs_and_get_volumes(run_dir, graphs_file, distance_name, distance_func):\n",
    "    \n",
    "    run_dir_cache_file = run_dir / \"volumes_cache.json\"\n",
    "    \n",
    "    if run_dir_cache_file.is_file():\n",
    "        volumes_cache_dict = json.load(open(run_dir_cache_file))\n",
    "        vol_1k, vol_1hundred = volumes_cache_dict[\"vol_1k\"], volumes_cache_dict[\"vol_1hundred\"]\n",
    "        \n",
    "        # print(\"Got values from cache\")\n",
    "    else:\n",
    "        thousand_graphs = np.load(graphs_file, allow_pickle=True)\n",
    "        graph_objects = get_graph_objects(thousand_graphs, distances_set={distance_name})[distance_name]\n",
    "        \n",
    "        _,  vol_1k = count_pairwise_energy(graph_objects, distance_function=distance_func)\n",
    "        _, vol_1hundred = count_pairwise_energy(graph_objects[:100], distance_function=distance_func)\n",
    "\n",
    "        volumes_cache_dict = dict(vol_1k=vol_1k, vol_1hundred=vol_1hundred)\n",
    "        json.dump(volumes_cache_dict, open(run_dir_cache_file, \"w\"))        \n",
    "    \n",
    "    return vol_1k, vol_1hundred\n",
    "\n",
    "def get_df_from_digress_iterations(root: Path, distance_func, distance_name:str):\n",
    "    data = []\n",
    "    for run_dir in root.glob(\"./*/\"):\n",
    "        \n",
    "        run_number = int(run_dir.name.split(\"_\")[-1]) + 1\n",
    "        graphs_file = run_dir / \"final_graphs_greedy.npy\"\n",
    "        \n",
    "        if not graphs_file.is_file():\n",
    "            continue\n",
    "        \n",
    "        vol_1k, vol_1hundred = load_graphs_and_get_volumes(run_dir, graphs_file, distance_name, distance_func)\n",
    "        \n",
    "        data.append([run_number, vol_1k, vol_1hundred])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"Step\", \"Fitness of 1k graphs\", \"Fitness of 100 graphs\"]).sort_values(by=\"Step\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_for_digress_runs(runs_roots_dict: dict[str, list[Path]], \n",
    "                                   distance_name:str,\n",
    "                                   zero_step_filename_location: Optional[str]=None,\n",
    "                                   ):\n",
    "    \n",
    "    \n",
    "    _columns = [\"Label\", \"ID\", \"Step\", \"Fitness of 1k graphs\", \"Fitness of 100 graphs\"]\n",
    "    \n",
    "    dataframes: list[pd.DataFrame] = []\n",
    "    \n",
    "    distance_function = partial(energy_distance, distance_name=distance_name)\n",
    "    \n",
    "    _labels_to_ids = defaultdict(list)\n",
    "    \n",
    "    for runs_label, runs_roots in runs_roots_dict.items():\n",
    "        \n",
    "        for i, run_root in enumerate(runs_roots):\n",
    "            \n",
    "            run_df = get_df_from_digress_iterations(root=run_root, distance_func=distance_function, distance_name=distance_name)\n",
    "            run_df[\"ID\"] = i\n",
    "            run_df[\"Label\"] = runs_label\n",
    "            \n",
    "            dataframes.append(run_df)\n",
    "            \n",
    "            _labels_to_ids[runs_label].append(i)\n",
    "       \n",
    "    if zero_step_filename_location is not None:\n",
    "        cache_file: Path = Path(zero_step_filename_location + \".json\")\n",
    "        \n",
    "        if cache_file.is_file():\n",
    "            cache = json.load(open(cache_file))\n",
    "            vol_1k, vol_1hundred = cache[\"vol_1k\"], cache[\"vol_1hundred\"]\n",
    "        else:\n",
    "            vol_1k, vol_1hundred = load_graphs_and_get_volumes(Path(\"./\"), zero_step_filename_location, distance_func=distance_function, distance_name=distance_name)\n",
    "            cache = dict(vol_1k=vol_1k, vol_1hundred=vol_1hundred)\n",
    "            json.dump(cache, open(cache_file, \"w\"))\n",
    "        \n",
    "        zero_step_data = []\n",
    "        \n",
    "        for _label, _ids in _labels_to_ids.items():\n",
    "            for _id in _ids:\n",
    "                zero_step_data.append(\n",
    "                    [_label, _id, 0, vol_1k, vol_1hundred]\n",
    "                )\n",
    "\n",
    "        dataframes.append(pd.DataFrame(data=zero_step_data,\n",
    "                                       columns=_columns)\n",
    "                          )\n",
    "            \n",
    "    dataframe: pd.DataFrame = pd.concat(dataframes).sort_values(by=[\"Step\"]).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    return dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcd_roots = {\"Memory_preserving\": [Path(\"/home/fvelikon/projects/DiGress/digress_iterations/GCD_preserve_memory\")],\n",
    "             \"Simple\": [Path(\"/home/fvelikon/projects/DiGress/digress_iterations/GCD\")]}\n",
    "\n",
    "gcd_dataframes = get_dataframe_for_digress_runs(gcd_roots, \"GCD\", \n",
    "                                                # zero_step_filename_location=\"/home/fvelikon/projects/DiGress/GCD_greedy_1k_graphs.pkl\",\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcd_dataframes.query(\"Label == 'Memory_preserving'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcd_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(gcd_dataframes, x=\"Step\", y=[\"Fitness of 100 graphs\"],\n",
    "        color=\"Label\",\n",
    "        markers=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portrait_roots = {\n",
    "            # \"Memory_preserving\": [Path(\"/home/fvelikon/projects/DiGress/digress_iterations/Portrait_memory_preserving\")],\n",
    "             \"Simple\": [Path(\"/home/fvelikon/projects/DiGress/digress_iterations/Portrait\")],\n",
    "             }\n",
    "\n",
    "portrait_dataframes = get_dataframe_for_digress_runs(portrait_roots, \"Portrait\", \n",
    "                                                zero_step_filename_location=\"/home/fvelikon/projects/DiGress/Portrait_greedy_1k_graphs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(portrait_dataframes, x=\"Step\", y=[\"Fitness of 100 graphs\", \"Fitness of 1k graphs\"],\n",
    "        # color=\"Label\",\n",
    "        markers=True,\n",
    "        title=\"Portrait\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_roots = {\n",
    "            # \"Memory_preserving\": [Path(\"/home/fvelikon/projects/DiGress/digress_iterations/Portrait_memory_preserving\")],\n",
    "             \"Simple\": [Path(\"/home/fvelikon/projects/DiGress/digress_iterations/heat\")],\n",
    "             }\n",
    "\n",
    "heat_dataframes = get_dataframe_for_digress_runs(heat_roots, \"netLSD_heat\", \n",
    "                                                zero_step_filename_location=\"/home/fvelikon/projects/DiGress/netLSD_heat_greedy_1k_graphs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(heat_dataframes, x=\"Step\", y=[\"Fitness of 100 graphs\", \"Fitness of 1k graphs\"],\n",
    "        # color=\"Label\",\n",
    "        markers=True,\n",
    "        title=\"netLSD_heat\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_roots = {\n",
    "            # \"Memory_preserving\": [Path(\"/home/fvelikon/projects/DiGress/digress_iterations/Portrait_memory_preserving\")],\n",
    "             \"Simple\": [Path(\"/home/fvelikon/projects/DiGress/digress_iterations/wave\")],\n",
    "             }\n",
    "\n",
    "wave_dataframes = get_dataframe_for_digress_runs(wave_roots, \"netLSD_wave\", \n",
    "                                                zero_step_filename_location=\"/home/fvelikon/projects/DiGress/netLSD_wave_greedy_1k_graphs.pkl\")\n",
    "display(wave_dataframes)\n",
    "px.line(wave_dataframes, x=\"Step\", y=[\"Fitness of 100 graphs\", \"Fitness of 1k graphs\"],\n",
    "        # color=\"Label\",\n",
    "        markers=True,\n",
    "        title=\"netLSD_wave\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_graphs_gcd = [nx.from_numpy_array(g) for g in np.load(\"/home/fvelikon/projects/DiGress/digress_iterations/GCD_preserve_memory/digress_run_9/final_graphs_greedy.npy\")][:100]\n",
    "\n",
    "\n",
    "\n",
    "graph_sorted_by_density = list(sorted(last_graphs_gcd, key=lambda x: nx.density(x), reverse=True))\n",
    "complements = list(map(nx.complement, graph_sorted_by_density))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graphs(last_graphs_gcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graphs(graph_sorted_by_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graphs(complements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_input_graphs_path = \"./GCD_train_val_1k_each.npz\" #\"./GCD_greedy_1k_graphs.pkl\"\n",
    "\n",
    "diffusion_input_graphs_path_1 = \"./GCD_greedy_1k.npy\"\n",
    "diffusion_input_graphs_path_2 = \"/home/fvelikon/projects/DiGress/graphs_1k_train_val_as_part_of_train/greedy_graphs_1k_v1.npy\" \n",
    "\n",
    "# final_graphs = np.load(\"./graphs_1k_train_val_as_part_of_train_RUN_2/final_graphs_greedy.pkl\", allow_pickle=True)[\"GCD\"]\n",
    "\n",
    "final_graphs = np.load(\"/home/fvelikon/projects/DiGress/graphs_1k_train_val_as_part_of_train_RUN_3/final_graphs_greedy.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion_input_graphs = np.load(diffusion_input_graphs_path, allow_pickle=True)[\"train\"]\n",
    "\n",
    "input_graphs_1 = np.load(diffusion_input_graphs_path_1, allow_pickle=True)\n",
    "input_graphs_2 = np.load(diffusion_input_graphs_path_2, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_graphs = read_pickle(\"./graphs/train_eval_test_1k_each/final_graphs_SUB_greedy.pkl\")[\"GCD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_graph_objects = get_graph_objects(model_10k_graphs, distances_set={\"GCD\"})\n",
    "\n",
    "# generated_graph_objects = get_graph_objects(generated_graphs, distances_set={\"GCD\"})\n",
    "input_graphs_1_graph_objects = get_graph_objects(input_graphs_1, distances_set={\"GCD\"})\n",
    "input_graphs_2_graph_objects = get_graph_objects(input_graphs_2, distances_set={\"GCD\"})\n",
    "final_grap_objects = get_graph_objects(final_graphs, distances_set={\"GCD\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_graphs = read_pickle(\"./generated_graphs_by_model_GCD_1M.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_graph_objects = get_graph_objects(diffusion_input_graphs, distances_set={\"GCD\"})\n",
    "# generated_graphs = get_graph_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, vol_1 = count_pairwise_energy(input_graphs_1_graph_objects[\"GCD\"], distance_function=GCD)\n",
    "\n",
    "# print(vol_1)\n",
    "# _, vol_2 = count_pairwise_energy(input_graphs_2_graph_objects[\"GCD\"], distance_function=GCD)\n",
    "\n",
    "# print(vol_2)\n",
    "\n",
    "_, vol_3 = count_pairwise_energy(final_grap_objects[\"GCD\"], distance_function=GCD)\n",
    "\n",
    "print(vol_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, vol_1 = count_pairwise_energy(input_graphs_1_graph_objects[\"GCD\"][:100], distance_function=GCD)\n",
    "\n",
    "# print(vol_1)\n",
    "# _, vol_2 = count_pairwise_energy(input_graphs_2_graph_objects[\"GCD\"][:100], distance_function=GCD)\n",
    "\n",
    "# print(vol_2)\n",
    "\n",
    "_, vol_3 = count_pairwise_energy(final_grap_objects[\"GCD\"][:100], distance_function=GCD)\n",
    "\n",
    "print(vol_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graphs([nx.from_numpy_array(G) for G in final_graphs[:100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes = []\n",
    "\n",
    "for i in range(10):\n",
    "    generated_graphs_sampled = get_graph_objects(random.sample(all_graphs, 1000), distances_set={\"GCD\"})[\"GCD\"]\n",
    "    _, vol =  count_pairwise_energy(generated_graphs_sampled, distance_function=GCD)\n",
    "    \n",
    "    print(vol)\n",
    "    \n",
    "    volumes.append(vol)\n",
    "\n",
    "\n",
    "volumns = np.array(volumes)\n",
    "\n",
    "volumes.mean(), volumes.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_100_graphs = [nx.from_numpy_array(g) for g in random.sample(model_10k_graphs, 100)]\n",
    "draw_graphs(random_100_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Greedy sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_greedy_from_graphobjects_of_certain_distance(graph_objects:List[GraphObject],\n",
    "                                                        distance_function:Callable[[Any, Any], float],\n",
    "                                                        final_set_size:int=100,\n",
    "                                                        super_greedy=False,\n",
    "                                                        ):    \n",
    "    \n",
    "    \n",
    "    if not super_greedy:\n",
    "        N = len(graph_objects)\n",
    "        competitors_per_sample = N // (final_set_size - 1)\n",
    "        \n",
    "        # random_permutation\n",
    "        indices = np.random.permutation(range(N))\n",
    "        \n",
    "        \n",
    "        offset = 2\n",
    "        resulting_set: List[GraphObject] = [graph_objects[random.choice(indices[:offset])]]\n",
    "        \n",
    "        with Parallel(n_jobs=12) as workers:\n",
    "            for i in range(final_set_size - 1):\n",
    "                \n",
    "                candidate_indices = indices[offset + competitors_per_sample * i : offset + competitors_per_sample * (i + 1)]\n",
    "                candidates = [graph_objects[k] for k in candidate_indices]\n",
    "                \n",
    "                \n",
    "                distances = np.array(workers(\n",
    "                    delayed(\n",
    "                    distance_function \n",
    "                    )(already_chosen_graph, candidate) for candidate in candidates for already_chosen_graph in resulting_set\n",
    "                ))\n",
    "                \n",
    "                distances = -1.0 * distances.reshape(len(candidate_indices), -1)\n",
    "                \n",
    "                fitnesses = distances.sum(1)\n",
    "                \n",
    "                max_fitness_index = fitnesses.argmax()\n",
    "                \n",
    "                \n",
    "                winner = candidates[max_fitness_index]\n",
    "                \n",
    "                resulting_set.append(winner)\n",
    "    else:\n",
    "        N = len(graph_objects)\n",
    "        \n",
    "        # random_permutation\n",
    "        indices = np.random.permutation(range(N))\n",
    "        \n",
    "        \n",
    "        offset = 2\n",
    "        resulting_set: List[GraphObject] = [graph_objects[0]]\n",
    "        \n",
    "        graphs = graph_objects[1:]\n",
    "        \n",
    "        fitnesses = np.zeros(len(graphs))\n",
    "        \n",
    "        with Parallel(n_jobs=12) as workers:\n",
    "            for i in trange(final_set_size - 1):\n",
    "                \n",
    "                \n",
    "                distances = np.array(workers(\n",
    "                    delayed(\n",
    "                    distance_function \n",
    "                    )(resulting_set[-1], candidate) for candidate in graphs\n",
    "                ))\n",
    "                \n",
    "                distances = distances.reshape(len(graphs), -1)\n",
    "                \n",
    "                fitnesses += distances.sum(1)\n",
    "                \n",
    "                max_fitness_index = fitnesses.argmin()\n",
    "                \n",
    "                winner = graphs[max_fitness_index]\n",
    "                fitnesses[max_fitness_index] += 1e5\n",
    "                \n",
    "                resulting_set.append(winner)\n",
    "\n",
    "\n",
    "    return resulting_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_greedy_graphs_for_generated_set(graph_objects_dict:Dict[str, GraphObject],\n",
    "                                             greedy_set_size:int,\n",
    "                                             number_of_repeats:int=5,\n",
    "                                             super_greedy:bool=False,\n",
    "                                             ):\n",
    "    \n",
    "    table = defaultdict(list)\n",
    "    final_graphs = {}\n",
    "    M = number_of_repeats\n",
    "\n",
    "    for distance_name, graph_objects_list in graph_objects_dict.items():\n",
    "        distance_func = partial(energy_distance, distance_name=distance_name)\n",
    "        \n",
    "        max_fitness = -1\n",
    "        for i in range(M):\n",
    "            greedy_chosen_graphs = sample_greedy_from_graphobjects_of_certain_distance(graph_objects_list, \n",
    "                                                                                        distance_function=distance_func,\n",
    "                                                                                        super_greedy=super_greedy,\n",
    "                                                                                        final_set_size=greedy_set_size,\n",
    "                                                                                        )\n",
    "            \n",
    "            distances, fitness = count_pairwise_energy(greedy_chosen_graphs, distance_func)\n",
    "            \n",
    "            if fitness > max_fitness:\n",
    "                final_graphs[distance_name] = [g.graph for g in greedy_chosen_graphs]\n",
    "                max_fitness = fitness\n",
    "                \n",
    "            \n",
    "            table[distance_name].append(fitness)\n",
    "            print(f\"{distance_name} - {i+1}/{M}\")\n",
    "            \n",
    "        \n",
    "        print(f\"{distance_name} - done\")\n",
    "        \n",
    "    overall_table = pd.DataFrame.from_dict(table)\n",
    "\n",
    "\n",
    "    cols = [\"fitness\", \"distance\"]\n",
    "    data = []\n",
    "    for d, array in table.items():\n",
    "        for f in array:\n",
    "            data.append([f, d])\n",
    "            \n",
    "            \n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "    distance_fitness_average_std = df.groupby(\"distance\").aggregate([\"mean\", \"std\"])\n",
    "    \n",
    "    return final_graphs, overall_table, distance_fitness_average_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_graphs, overall_table, distance_fitness_average_std = generate_greedy_graphs_for_generated_set(\n",
    "    graph_objects_dict=get_graph_objects(all_graphs, distances_set={\"GCD\"})[\"GCD\"],\n",
    "    greedy_set_size=1000,\n",
    "    number_of_repeats=5,\n",
    "    super_greedy=False,\n",
    ")\n",
    "\n",
    "display(overall_table)\n",
    "display(distance_fitness_average_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_graphs, overall_table_super_greedy, distance_fitness_average_std_super_greedy = generate_greedy_graphs_for_generated_set(\n",
    "    graph_objects_dict=generated_graph_objects,\n",
    "    greedy_set_size=100,\n",
    "    number_of_repeats=1,\n",
    "    super_greedy=True,\n",
    ")\n",
    "\n",
    "display(overall_table_super_greedy)\n",
    "display(distance_fitness_average_std_super_greedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sampling from graph random models greedily for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from generation import get_initial_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?get_initial_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_configurations = json.load(open(\"../graph_diversity_problems/configs/config_final.json\"))[\"models\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OF_GRAPHS = 10000\n",
    "NUM_OF_GRAPHS_GREEDY = 1000\n",
    "\n",
    "NUM_OF_NODES = 16\n",
    "\n",
    "THREADS = 12\n",
    "\n",
    "DISTANCES = {\"GCD\"}\n",
    "\n",
    "greedy_graphs_objects_per_distance = None\n",
    "orca_path=\"../graph_diversity_problems/orca/\"\n",
    "\n",
    "equal_sizes=True\n",
    "maybe_ready_graphs=None\n",
    "\n",
    "config = {\"initial_graphs\": [\"mix\"], \n",
    "          \"models\": models_configurations,\n",
    "          \"greedy_sampling_size\": NUM_OF_GRAPHS,\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_initial_graphs_partial = partial(get_initial_graphs,\n",
    "                             config=config,\n",
    "                             threads=THREADS,\n",
    "                             distances_set=DISTANCES,\n",
    "                             greedy_graphs_objects_per_distance=greedy_graphs_objects_per_distance,\n",
    "                             samples=NUM_OF_GRAPHS,\n",
    "                             nodes_number=NUM_OF_NODES,\n",
    "                             orca_path=orca_path,\n",
    "                             equal_sizes=equal_sizes,\n",
    "                             maybe_ready_graphs=maybe_ready_graphs\n",
    "                             \n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_initial_set_from_mix(generation_func,\n",
    "                                       distance_name=\"GCD\",\n",
    "                                       num_of_samples=1000,\n",
    "                                       super_greedy=True,\n",
    "                                       ):\n",
    "    \n",
    "    initial_graph_objects = get_graph_objects(get_initial_graphs_func=generation_func, \n",
    "                                              distances_set=DISTANCES)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    final_graphs, _, _ = generate_greedy_graphs_for_generated_set(\n",
    "                graph_objects_dict=initial_graph_objects,\n",
    "                greedy_set_size=num_of_samples,\n",
    "                number_of_repeats=1,\n",
    "                super_greedy=super_greedy,\n",
    "            )\n",
    "    \n",
    "    \n",
    "    return final_graphs[distance_name]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_graphs_1 = create_greedy_initial_set_from_mix(get_initial_graphs_partial)\n",
    "initial_graphs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_graphs_2 = create_greedy_initial_set_from_mix(get_initial_graphs_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_set = {\n",
    "    \"train\": initial_graphs_1,\n",
    "    \"valid\": initial_graphs_2,\n",
    "    \"test\": initial_graphs_2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\n",
    "    \"GCD_train_val_1k_each\",\n",
    "    **graphs_set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"train\" in np.load(\"./GCD_train_val_1k_each.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Get 1k graphs for train and for test independently from the same graph random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = []\n",
    "\n",
    "get_initial_graphs_partial_with_size_100 = partial(get_initial_graphs,\n",
    "                             config=config,\n",
    "                             threads=THREADS,\n",
    "                             distances_set=DISTANCES,\n",
    "                             greedy_graphs_objects_per_distance=greedy_graphs_objects_per_distance,\n",
    "                             samples=NUM_OF_GRAPHS,\n",
    "                             nodes_number=NUM_OF_NODES,\n",
    "                             orca_path=orca_path,\n",
    "                             equal_sizes=equal_sizes,\n",
    "                             maybe_ready_graphs=maybe_ready_graphs\n",
    "                             \n",
    "                             )\n",
    "\n",
    "for i in range(10):\n",
    "    _graphs, _, _ = create_greedy_initial_set_from_mix(get_initial_graphs_partial, num_of_samples=100)\n",
    "    \n",
    "    graphs.extend(_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "digress",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
